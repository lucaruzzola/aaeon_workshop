{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aim2 workshop - From models to complete solutions using OpenVINO \n",
    "[![aim2](./assets/aim2.png)](https://www.youtube.com/watch?v=a6bwjYjuBEg)\n",
    "\n",
    "### Luca Ruzzola, Machine Learning Engineer @ aim2.io\n",
    "\n",
    "### What is computer vision\n",
    "*\"**Computer vision** is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"*\n",
    "\n",
    "### What is deep learning\n",
    "*\"**Deep Learning** is a class of machine learning algorithms that use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation to learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts.\"*\n",
    "\n",
    "*\"**Machine Learning** is the discipline that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\"*\n",
    "\n",
    "*\"**AI** is the discipline dealing with the designing and building of intelligent agents that receive percepts from the environment and take actions that affect that environment.\"*\n",
    "\n",
    "![Deep learning](./assets/deep_learning.png)\n",
    "\n",
    "### What is a CNN\n",
    "A **CNN** is a neural network that uses kernel convolution instead of matrix multiplication in one or more of its layers.\n",
    "![Convolution](./assets/convolution.gif)\n",
    "![Application of a blur filter](./assets/filterd_image.png)\n",
    "![Application of an edge detection filter](./assets/cameraman.png)\n",
    "\n",
    "## Let's get our hands dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "Please clone this repository: https://github.com/lucaruzzola/aaeon_workshop\n",
    "\n",
    "*git clone https://github.com/lucaruzzola/aaeon_workshop*\n",
    "\n",
    "If you don't have git installed you can install it executing this command in a terminal:\n",
    "*sudo apt-get install git*\n",
    "\n",
    "Create the Anaconda environment as such: *conda env create -f workshopenv.yml*\n",
    "\n",
    "If you don't have Anaconda installed alredy you can download it from: https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n",
    "\n",
    "To test that everything is working please type in a terminal: \n",
    "\n",
    "*source activate workshop*\n",
    "\n",
    "*source /opt/intel/computer_vision_sdk/bin/setupvars.sh*\n",
    "\n",
    "*python blur.py*\n",
    "\n",
    "If everything is working as expected you should see the live video from your camera with blurred faces.\n",
    "\n",
    "You can then quit the demo by pressing \"q\" and start the notebook for this session typing in the same terminal:\n",
    "\n",
    "*python -m jupyter notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import openvino\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire images from you camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "try:\n",
    "    ret, img = cap.read()\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection\n",
    "\n",
    "Traditional techniques, like Viola-Jones used custom-engineered features, and work quite well without the need to train them, however they are quite brittle and make somewhat strict assumptions.\n",
    "Modern techniques like MobileNetSSD are more precise and particularly more resilient, they can account for more variation in a face and its position in the image.\n",
    "\n",
    "MobileNetSSD is a very popular CNN architecture for general object detection, especially used on low power devices, and as every deep learning model requires quite a bit of expertise to train and deploy.\n",
    "However thanks to OpenVINO and its model zoo it's nowadays possible to use it just like any other library you encounter in you daily workflow.\n",
    "\n",
    "You can simply load the model and use to get the bounding boxes of every face in an image, however there is still quite a bit of code that you need to write to be able to use it effectively and even more easily, and we will later see how easy this can get when you have the necessary tools in place, to be able to go from thinking about a single model, to thinking about a complete AI solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from openvino.inference_engine import IEPlugin, IENetwork\n",
    "from utils import SyRegion, SyFrame, Location, draw_bounding_box\n",
    "\n",
    "#Load a model using plain OpenVINO\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "try: \n",
    "    device = \"CPU\"\n",
    "    cpu_extension = \"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\"\n",
    "    plugin = IEPlugin(device=device, plugin_dirs=None)\n",
    "    if cpu_extension and 'CPU' in device:\n",
    "        plugin.add_cpu_extension(cpu_extension)\n",
    "\n",
    "    net = IENetwork(model=face_xml, weights=face_bin)\n",
    "\n",
    "    input_layer = next(iter(net.inputs))\n",
    "    output_layer = next(iter(net.outputs))\n",
    "    exec_net = plugin.load(network=net, num_requests=1)\n",
    "    n, c, net_input_height, net_input_width = net.inputs[input_layer].shape\n",
    "    del net\n",
    "\n",
    "    #pre\n",
    "    copy_frame = copy.deepcopy(frame)\n",
    "    resized_frame = cv2.resize(copy_frame, (300, 300))\n",
    "    transposed_frame = resized_frame.transpose((2,0,1))\n",
    "\n",
    "    #detection\n",
    "    network_output = exec_net.infer(inputs={input_layer: transposed_frame})[output_layer]\n",
    "\n",
    "    #post\n",
    "    thr=0.5\n",
    "\n",
    "    for obj in network_output[0][0]:\n",
    "        textual_label = str(int(obj[1]))\n",
    "        confidence = obj[2]\n",
    "\n",
    "        if int((obj[1])) != -1 and confidence > thr:\n",
    "            x_min = max(0, int(obj[3] * 1280))\n",
    "            y_min = max(0, int(obj[4] * 720))\n",
    "            x_max = int(obj[5] * 1280)\n",
    "            y_max = int(obj[6] * 720)\n",
    "\n",
    "            face = SyRegion(label=textual_label, confidence=confidence, location=Location(x=x_min, y=y_min, w=x_max - x_min, h=y_max - y_min), sy_frame=SyFrame(frame))\n",
    "\n",
    "            draw_bounding_box(face,frame,color=(50,50,200),thickness=8)\n",
    "    plt.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    plt.show\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FaceDetector\n",
    "\n",
    "face_xml=\"./assets/face_detection/FP32/fd.xml\"\n",
    "face_bin=\"./assets/face_detection/FP32/fd.bin\"\n",
    "\n",
    "face_detector = FaceDetector(model_xml=face_xml,\\\n",
    "                             model_bin=face_bin,\\\n",
    "                             device=\"CPU\",\\\n",
    "                             confidence_threshold=0.8,\\\n",
    "                             cpu_extension=\"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, img = cap.read()\n",
    "detected_faces=face_detector.detect(SyFrame(img))\n",
    "\n",
    "try:\n",
    "    for face in detected_faces:\n",
    "\n",
    "        draw_bounding_box(\\\n",
    "        face,\\\n",
    "        img,\\\n",
    "        )\n",
    "\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blur the face\n",
    "We are now going to use OpenCV to blur the faces that we have just detected, in order to reproduce the same result that you saw before and therefore to build a privacy-preserving system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, img = cap.read()\n",
    "detected_faces=face_detector.detect(SyFrame(img))\n",
    "try:\n",
    "    for face in detected_faces:\n",
    "        sq_loc = face.get_square_location()\n",
    "        blur_face = face.get_square_frame_region().frame\n",
    "        blur_face = cv2.blur(blur_face,(55,55))\n",
    "        draw_bounding_box(face,img)\n",
    "\n",
    "        img[sq_loc.y:sq_loc.y+blur_face.shape[0], sq_loc.x:sq_loc.x+blur_face.shape[1]] = blur_face\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion recognition\n",
    "Now we are going to add emotion recognition to our system, so that we can show an emoji in real time, matching people's expressions, instead of just blurring.\n",
    "\n",
    "We are again going to use a pre-trained CNN that has been trained for this very specific task, to be able to distinguish between 5 different expressions: neutral, happy, sad, surprised, angry.\n",
    "\n",
    "The output is going to be something like this:\n",
    "![Emoji result](./assets/emoji_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EmotionClassifier\n",
    "from utils import load_emojis \n",
    "from utils import emoji_overlay\n",
    "\n",
    "emotion_classifier = EmotionClassifier(model_xml=\"./assets/emotion_recognition/FP32/em.xml\",\\\n",
    "                                       model_bin=\"./assets/emotion_recognition/FP32/em.bin\",\\\n",
    "                                       device=\"CPU\",\\\n",
    "                                       cpu_extension=\"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\",\\\n",
    "                                       emotion_label_list=[\"neutral\", \"happy\", \"sad\", \"surprise\", \"anger\"])\n",
    "\n",
    "emojis = load_emojis(\"./assets/emojis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, img = cap.read()\n",
    "detected_faces=face_detector.detect(SyFrame(img))\n",
    "\n",
    "try:\n",
    "    for face in detected_faces:\n",
    "        emotion = emotion_classifier.predict(face.get_square_frame_region().frame)\n",
    "        emoji_overlay(emojis[emotion], img, face.location)\n",
    "        draw_bounding_box(face,img)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
