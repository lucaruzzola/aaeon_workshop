{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# aim2 workshop - From models to complete solutions using OpenVINO \n",
    "[![aim2](./assets/aim2.png)](https://www.youtube.com/watch?v=a6bwjYjuBEg)\n",
    "\n",
    "### Luca Ruzzola, Machine Learning Engineer @ aim2.io\n",
    "\n",
    "### What is computer vision\n",
    "*\"**Computer vision** is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"*\n",
    "\n",
    "### What is deep learning\n",
    "*\"**Deep Learning** is a class of machine learning algorithms that use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation to learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts.\"*\n",
    "\n",
    "*\"**Machine Learning** is the discipline that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\"*\n",
    "\n",
    "*\"**AI** is the discipline dealing with the designing and building of intelligent agents that receive percepts from the environment and take actions that affect that environment.\"*\n",
    "\n",
    "![Deep learning](./assets/deep_learning.png)\n",
    "\n",
    "### What is a CNN\n",
    "A **CNN** is a neural network that uses kernel convolution instead of matrix multiplication in one or more of its layers.\n",
    "![Convolution](./assets/convolution.gif)\n",
    "![Application of a blur filter](./assets/filterd_image.png)\n",
    "![Application of an edge detection filter](./assets/cameraman.png)\n",
    "\n",
    "## Let's get our hands dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "Please clone this repository: https://github.com/lucaruzzola/aaeon_workshop\n",
    "\n",
    "*git clone https://github.com/lucaruzzola/aaeon_workshop*\n",
    "\n",
    "If you don't have git installed you can install it executing this command in a terminal:\n",
    "*sudo apt-get install git*\n",
    "\n",
    "Create the Anaconda environment as such: *conda env create -f workshopenv.yml*\n",
    "\n",
    "If you don't have Anaconda installed already you can download it from: https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n",
    "\n",
    "To test that everything is working please type in a terminal: \n",
    "\n",
    "*source activate workshop*\n",
    "\n",
    "*source /opt/intel/computer_vision_sdk/bin/setupvars.sh*\n",
    "\n",
    "*python blur.py*\n",
    "\n",
    "If everything is working as expected you should see the live video from your camera with blurred faces.\n",
    "\n",
    "You can then quit the demo by pressing \"q\" and start the notebook for this session typing in the same terminal:\n",
    "\n",
    "*python -m jupyter notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import openvino\n",
    "import copy\n",
    "from openvino.inference_engine import IEPlugin, IENetwork\n",
    "from utils import SyRegion, SyFrame, Location, draw_bounding_box\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire images from you camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try to use opencv's VideoCapture class and plt.show to capture and display an image from your camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection\n",
    "\n",
    "Traditional techniques, like Viola-Jones used custom-engineered features, and work quite well without the need to train them, however they are quite brittle and make somewhat strict assumptions.\n",
    "Modern techniques like MobileNetSSD are more precise and particularly more resilient, they can account for more variation in a face and its position in the image.\n",
    "\n",
    "MobileNetSSD is a very popular CNN architecture for general object detection, especially used on low power devices, and as every deep learning model requires quite a bit of expertise to train and deploy.\n",
    "However thanks to OpenVINO and its model zoo it's nowadays possible to use it just like any other library you encounter in you daily workflow.\n",
    "\n",
    "You can simply load the model and use to get the bounding boxes of every face in an image, however there is still quite a bit of code that you need to write to be able to use it effectively and even more easily, and we will later see how easy this can get when you have the necessary tools in place, to be able to go from thinking about a single model, to thinking about a complete AI solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FaceDetector\n",
    "\n",
    "face_xml=\"./assets/face_detection/FP32/fd.xml\"\n",
    "face_bin=\"./assets/face_detection/FP32/fd.bin\"\n",
    "\n",
    "face_detector = FaceDetector(model_xml=face_xml,\\\n",
    "                             model_bin=face_bin,\\\n",
    "                             device=\"CPU\",\\\n",
    "                             confidence_threshold=0.8,\\\n",
    "                             cpu_extension=\"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the utilities we provided to detect all faces in a image and to display a box around them\n",
    "#In particular, take a loook at draw_bounding_box, plt.imshow, and the face_detector class\n",
    "#Before feeding the frame to the detector please wrap it using SyFrame e.g. wrapped_image = SyFrame(img)\n",
    "\n",
    "#Bonus: try to change the confidence threshold in the previous block and see the different results you get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blur the face\n",
    "We are now going to use OpenCV to blur the faces that we have just detected, in order to reproduce the same result that you saw before and therefore to build a privacy-preserving system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that you are able to get all faces from a given image, let's blur them\n",
    "#To do this, take a look at the cv2.blur function\n",
    "#You can then display a modified image where you changed the original pixels with blurried ones\n",
    "#To change the pixels to the blurried ones you can do something like this\n",
    "# img[sq_loc.y:sq_loc.y+blur_face.shape[0], sq_loc.x:sq_loc.x+blur_face.shape[1]] = blur_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion recognition\n",
    "Now we are going to add emotion recognition to our system, so that we can show an emoji in real time, matching people's expressions, instead of just blurring.\n",
    "\n",
    "We are again going to use a pre-trained CNN that has been trained for this very specific task, to be able to distinguish between 5 different expressions: neutral, happy, sad, surprised, angry.\n",
    "\n",
    "The output is going to be something like this:\n",
    "![Emoji result](./assets/emoji_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EmotionClassifier\n",
    "from utils import load_emojis \n",
    "from utils import emoji_overlay\n",
    "\n",
    "emotion_classifier = EmotionClassifier(model_xml=\"./assets/emotion_recognition/FP32/em.xml\",\\\n",
    "                                       model_bin=\"./assets/emotion_recognition/FP32/em.bin\",\\\n",
    "                                       device=\"CPU\",\\\n",
    "                                       cpu_extension=\"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\",\\\n",
    "                                       emotion_label_list=[\"neutral\", \"happy\", \"sad\", \"surprise\", \"anger\"])\n",
    "\n",
    "emojis = load_emojis(\"./assets/emojis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's display an emoji with the current expression on people faces instead of a simple blur\n",
    "#Take a look at the emoji_overlay utility, and at the emotion_classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
